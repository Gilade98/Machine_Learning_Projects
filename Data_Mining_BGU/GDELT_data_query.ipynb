{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1mt-Rdzrx0MfeqRTeuQWSzb5KH6-F8Cr9",
      "authorship_tag": "ABX9TyOIhLvuNtksP1/UM2oRIgvB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gilade98/Machine_Learning_Projects/blob/main/Data_Mining_BGU/GDELT_data_query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from bs4 import BeautifulSoup\n",
        "import gc"
      ],
      "metadata": {
        "id": "AnIvf6WZhRuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastparquet pyarrow tqdm gdelt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPJEZ3_qBG3L",
        "outputId": "c9c51a8a-0f43-48a6-cc45-c2779ee26e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.11/dist-packages (2024.11.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (17.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: gdelt in /usr/local/lib/python3.11/dist-packages (0.1.14)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastparquet) (1.26.4)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2024.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet) (24.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from gdelt) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from gdelt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->gdelt) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->gdelt) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->gdelt) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->gdelt) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->gdelt) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdelt\n",
        "\n",
        "# GDELT GKG Index Page\n",
        "GDELT_INDEX_URL = \"http://data.gdeltproject.org/gkg/index.html\"\n",
        "\n",
        "# Define themes to filter\n",
        "TARGET_THEMES = {\n",
        "    \"ECON_HOUSING_PRICES\",\n",
        "    \"GENTRIFICATION\",\n",
        "    \"NEW_CONSTRUCTION\",\n",
        "    \"PROPERTY_RIGHTS\",\n",
        "    \"POPULATION_DENSITY\",\n",
        "    \"URBAN\",\n",
        "    \"URBAN_SPRAWL\",\n",
        "    \"POVERTY\"\n",
        "}\n",
        "\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/data_toolbox/datasets\"\n",
        "START_YEAR = 2015\n",
        "TIME_DELTA=13\n",
        "gd1 = gdelt.gdelt(version=1)\n"
      ],
      "metadata": {
        "id": "C90sWt0miK7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d71031-ee23-43d2-d6f9-e60538edffed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gdelt_data(start_date, end_date):\n",
        "    \"\"\"Retrieve GKG data for a date range using gdelt package.\"\"\"\n",
        "    date_list = [(start_date + timedelta(days=i)).strftime('%Y %m %d')\n",
        "                 for i in range((end_date - start_date).days + 1)]\n",
        "\n",
        "    # Fetch GKG data\n",
        "    results = gd1.Search(date=date_list, table='gkg', coverage=True, output='df')\n",
        "    return results\n",
        "\n",
        "\n",
        "def get_last_processed_date():\n",
        "    \"\"\"Find the earliest processed date from the last row of the first yearly Parquet file.\"\"\"\n",
        "\n",
        "    if not os.path.exists(OUTPUT_FOLDER):\n",
        "        return None  # No files exist yet\n",
        "\n",
        "    # Get the earliest file by sorting file names (assuming 'gdelt_YYYY.parquet' format)\n",
        "    parquet_files = sorted(f for f in os.listdir(OUTPUT_FOLDER) if f.endswith(\".parquet\"))\n",
        "\n",
        "    if not parquet_files:\n",
        "        return None  # No Parquet files found\n",
        "\n",
        "    earliest_file = os.path.join(OUTPUT_FOLDER, parquet_files[1])  # Pick the first (earliest year) skipping the large file\n",
        "\n",
        "    # Read only the last row to get the earliest date (efficient)\n",
        "    df = pd.read_parquet(earliest_file, columns=[\"DATE\"]).tail(1)  # Read last row\n",
        "\n",
        "    if df.empty:\n",
        "        return None  # No data in the file\n",
        "\n",
        "    # Convert DATE column and return the earliest date\n",
        "    earliest_date = pd.to_datetime(df[\"DATE\"].iloc[0], format=\"%d-%m-%Y\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    print(f\"Resuming from last processed date: {earliest_date}\")\n",
        "    return earliest_date\n",
        "\n",
        "\n",
        "def process_gkg_data(df):\n",
        "  \"\"\"Filter the DataFrame to retain only relevant themes and source URLs.\"\"\"\n",
        "  if df.empty:\n",
        "      return df\n",
        "\n",
        "  df = df.loc[:, [\"DATE\", \"THEMES\", \"SOURCEURLS\"]]\n",
        "  df = df.dropna(subset=[\"THEMES\", \"SOURCEURLS\"])\n",
        "\n",
        "  # Filter rows with target themes\n",
        "  df = df[df['THEMES'].apply(lambda x: any(theme in str(x).split(\";\") for theme in TARGET_THEMES))]\n",
        "\n",
        "  # Convert DATE column format\n",
        "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], format=\"%Y%m%d\").dt.strftime(\"%d-%m-%Y\")\n",
        "\n",
        "  return df.loc[:, [\"DATE\", \"SOURCEURLS\"]]\n",
        "\n",
        "def load_year_data(year):\n",
        "    year_file = f\"{OUTPUT_FOLDER}/gdelt_{year}.parquet\"\n",
        "    if os.path.exists(year_file):\n",
        "        return pd.read_parquet(year_file, columns=[\"DATE\", \"SOURCEURLS\"])\n",
        "    else:\n",
        "        return pd.DataFrame(columns=[\"DATE\", \"SOURCEURLS\"])  # Empty DataFrame\n"
      ],
      "metadata": {
        "id": "gjgJXnKgiZhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_date = get_last_processed_date()\n",
        "last_date"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_thANCMj8PTi",
        "outputId": "d3b0a89a-ef97-45aa-b65b-42dacb4757d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from last processed date: 2015-03-23\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2015-03-23'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# Filter out files that were already processed\n",
        "if last_date:\n",
        "    start_date = date.fromisoformat(last_date)\n",
        "else:\n",
        "    start_date = date.today()\n",
        "\n",
        "current_year = start_date.year\n",
        "year_file = f\"{OUTPUT_FOLDER}/gdelt_{current_year}.parquet\"\n",
        "year_data = load_year_data(current_year)\n",
        "\n",
        "total_iterations = (start_date - date(START_YEAR, 1, 1)).days // (TIME_DELTA+1)  # Total number of 2-week periods\n",
        "\n",
        "with tqdm(total=total_iterations, desc=\"Processing Batches\", unit=\"batch\") as pbar:\n",
        "  while start_date >= date(START_YEAR, 1, 1):  # Set a lower bound to avoid indefinite looping\n",
        "    end_date = start_date\n",
        "    start_date = start_date - timedelta(days=TIME_DELTA)  # 2-week window\n",
        "\n",
        "    print(f\"\\nFetching data from {start_date} to {end_date}...\")\n",
        "    raw_data = get_gdelt_data(start_date, end_date)\n",
        "    filtered_data = process_gkg_data(raw_data)\n",
        "\n",
        "    if not filtered_data.empty:\n",
        "        # Convert DATE column to datetime\n",
        "        filtered_data[\"DATE\"] = pd.to_datetime(filtered_data[\"DATE\"], format=\"%d-%m-%Y\")\n",
        "\n",
        "        # Get unique years in the new data\n",
        "        for year, batch_year_data in filtered_data.groupby(filtered_data[\"DATE\"].dt.year):\n",
        "          if year != current_year:\n",
        "                    # Save the previous year's data before switching\n",
        "                    year_data.to_parquet(year_file, index=False, compression=\"snappy\", engine=\"fastparquet\")\n",
        "                    print(f\"Saved {len(year_data)} rows to {year_file}\")\n",
        "\n",
        "                    # Free memory of the old year\n",
        "                    del year_data\n",
        "                    gc.collect()\n",
        "\n",
        "                    # Load new year data (or create an empty one)\n",
        "                    current_year = year\n",
        "                    year_file = f\"{OUTPUT_FOLDER}/gdelt_{current_year}.parquet\"\n",
        "                    year_data = load_year_data(current_year)\n",
        "          # Append batch data for the current year\n",
        "          year_data = pd.concat([year_data, batch_year_data], ignore_index=True)\n",
        "    # Save after each 2-week batch\n",
        "    year_data.to_parquet(year_file, index=False, compression=\"snappy\", engine=\"fastparquet\")\n",
        "    print(f\"\\nSaved batch {len(year_data)} rows to {year_file}\")\n",
        "    del filtered_data\n",
        "    del raw_data\n",
        "    gc.collect()\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "# Final save before exit\n",
        "year_data.to_parquet(year_file, index=False, compression=\"snappy\", engine=\"fastparquet\")\n",
        "print(f\"\\nFinal save for {current_year}: {len(year_data)} rows\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1zFSzsE8G86",
        "outputId": "c6a6ee5f-186d-4df5-99d8-428fef6413df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Batches:   0%|          | 0/5 [00:00<?, ?batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data from 2015-03-10 to 2015-03-23...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Batches:  20%|██        | 1/5 [01:51<07:26, 111.56s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved batch 3137756 rows to /content/drive/MyDrive/data_toolbox/gdelt_2015.parquet\n",
            "\n",
            "Fetching data from 2015-02-25 to 2015-03-10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Batches:  40%|████      | 2/5 [03:20<04:55, 98.41s/batch] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved batch 3259320 rows to /content/drive/MyDrive/data_toolbox/gdelt_2015.parquet\n",
            "\n",
            "Fetching data from 2015-02-12 to 2015-02-25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Batches:  60%|██████    | 3/5 [04:27<02:48, 84.13s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved batch 3352752 rows to /content/drive/MyDrive/data_toolbox/gdelt_2015.parquet\n",
            "\n",
            "Fetching data from 2015-01-30 to 2015-02-12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Batches:  80%|████████  | 4/5 [05:18<01:10, 70.80s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved batch 3416238 rows to /content/drive/MyDrive/data_toolbox/gdelt_2015.parquet\n",
            "\n",
            "Fetching data from 2015-01-17 to 2015-01-30...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Batches: 100%|██████████| 5/5 [06:10<00:00, 64.01s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved batch 3485743 rows to /content/drive/MyDrive/data_toolbox/gdelt_2015.parquet\n",
            "\n",
            "Fetching data from 2015-01-04 to 2015-01-17...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Batches: 6batch [06:59, 59.14s/batch]                    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved batch 3548136 rows to /content/drive/MyDrive/data_toolbox/gdelt_2015.parquet\n",
            "\n",
            "Fetching data from 2014-12-22 to 2015-01-04...\n",
            "Saved 3548136 rows to /content/drive/MyDrive/data_toolbox/gdelt_2015.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-77db8515a635>:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  year_data = pd.concat([year_data, batch_year_data], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 36940 rows to /content/drive/MyDrive/data_toolbox/gdelt_2014.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batches: 7batch [07:53, 67.58s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved batch 3559861 rows to /content/drive/MyDrive/data_toolbox/gdelt_2015.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final save for 2015: 3559861 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates_from_parquet():\n",
        "    \"\"\"Remove duplicates from all yearly Parquet files and overwrite them.\"\"\"\n",
        "\n",
        "    if not os.path.exists(OUTPUT_FOLDER):\n",
        "        print(\"No Parquet folder found.\")\n",
        "        return\n",
        "\n",
        "    # Select only files that match the 'gdelt_YYYY.parquet' format\n",
        "    parquet_files = sorted(f for f in os.listdir(OUTPUT_FOLDER) if f.endswith(\".parquet\"))\n",
        "\n",
        "    for file in parquet_files:\n",
        "        file_path = os.path.join(OUTPUT_FOLDER, file)\n",
        "\n",
        "        # Load Parquet file\n",
        "        df = pd.read_parquet(file_path)\n",
        "\n",
        "        # Drop duplicates based on DATE and SOURCEURLS\n",
        "        df = df.drop_duplicates(subset=[\"DATE\", \"SOURCEURLS\"])\n",
        "\n",
        "        # Save back to the same Parquet file (overwrite)\n",
        "        df.to_parquet(file_path, index=False, compression=\"snappy\", engine=\"fastparquet\")\n",
        "        print(f\"Updated {file}: Removed duplicates.\")\n",
        "\n",
        "        # Free memory\n",
        "        del df\n",
        "        gc.collect()\n",
        "\n",
        "# Run the function to clean all Parquet files\n",
        "remove_duplicates_from_parquet()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4PK143NfAfp",
        "outputId": "fd031d7e-2d0b-4afb-c094-de01598e480c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated filtered_gdelt_data.parquet: Removed duplicates.\n",
            "Updated gdelt_2014.parquet: Removed duplicates.\n",
            "Updated gdelt_2015.parquet: Removed duplicates.\n",
            "Updated gdelt_2016.parquet: Removed duplicates.\n",
            "Updated gdelt_2017.parquet: Removed duplicates.\n",
            "Updated gdelt_2018.parquet: Removed duplicates.\n",
            "Updated gdelt_2019.parquet: Removed duplicates.\n",
            "Updated gdelt_2020.parquet: Removed duplicates.\n",
            "Updated gdelt_2021.parquet: Removed duplicates.\n",
            "Updated gdelt_2022.parquet: Removed duplicates.\n",
            "Updated gdelt_2023.parquet: Removed duplicates.\n",
            "Updated gdelt_2024.parquet: Removed duplicates.\n",
            "Updated gdelt_2025.parquet: Removed duplicates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PzMrZ0rd-ylD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}