# -*- coding: utf-8 -*-
"""Articles_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I4TFg1wwXACMtu7Q4_D6v-XDzw0fmzrL
"""

import logging
import os
import random
from urllib.parse import urlparse
from multiprocessing import Pool
from pathlib import Path
from multiprocessing import TimeoutError
import pandas as pd
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time

logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)

DATASETS_DIR = ""
OUTPUT_DIR = ""

# A small rotating list of user agents
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko)",
    "Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.127 Mobile Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko)",
    # Add more if desired
]

BLOCKED_DOMAINS = []

def create_session():
    """
    Create a requests Session with:
      - Automatic retries on common network errors (ConnectionError, 5xx, etc.)
      - A random User-Agent for each new session
    """
    s = requests.Session()

    # Set up retry strategy (change max_retries if you want more attempts)
    retries = Retry(
        total=1,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504]
    )
    s.mount("http://", HTTPAdapter(max_retries=retries))
    s.mount("https://", HTTPAdapter(max_retries=retries))

    # Random user agent
    s.headers.update({"User-Agent": random.choice(USER_AGENTS)})

    return s

# --- GLOBAL (set in init_pool)
session = None
def init_pool():
    """
    Initializes a global session object in each worker process.
    This function runs once *per process* when the Pool starts.
    """
    global session
    session = create_session()


def scrape_article(row):
    """
    row: {'DATE': <str/int>, 'SOURCEURLS': <str>}
    session: a requests session pre-configured with random UA & retries.

    Returns a dict:
      {
         "DATE": ...,
         "SOURCEURLS": ...,
         "content": <str or None>,
         "error": <str or None>
      }
    """
    global session
    date = row["DATE"]
    url = row["SOURCEURLS"]
    domain = urlparse(url).netloc

    # Check if the domain is blocked
    if domain in BLOCKED_DOMAINS:
        return None  # Skip this request

    try:
        # Random small delay to avoid hitting servers in a burst
        time.sleep(random.uniform(0.05, 0.1))

        t0 = time.time()
        resp = session.get(url, timeout=10)
        t1 = time.time()
        if not resp.ok:
          # If we get non-2xx status, skip this URL
          return {
              "DATE": date,
              "SOURCEURLS": url,
              "content": None,
              "domain": domain,
              "time_taken": round(t1 - t0, 3),
              "status_code": resp.status_code
          }
        soup = BeautifulSoup(resp.text, 'html.parser')
        text = soup.get_text(separator=" ", strip=True)
        text = remove_surrogates(text)
        return {
            "DATE": date,
            "SOURCEURLS": url,
            "content": text,
            "domain": domain,
            "time_taken": round(t1 - t0, 3),
            "status_code": resp.status_code
        }

    except Exception:
        # If any error/timeout happens, skip this URL
        return None

def count_csv_data_lines(csv_path):
    """
    Return the number of data rows in a CSV.
    We subtract 1 for the header (assuming there's exactly 1 header line).
    """
    if not os.path.exists(csv_path):
        return 0
    with open(csv_path, 'r', encoding='utf-8', errors='replace') as f:
        total_lines = sum(1 for _ in f)
    # Subtract 1 for the header line, but don't go below 0
    return max(0, total_lines - 1)

def write_progress(progress_path, line_count):
    """
    Write the total 'line_count' of the CSV to a small text file
    so a consumer can know how many rows are ready.
    """
    with open(progress_path, 'w') as f:
        f.write(str(line_count))

def remove_surrogates(s: str) -> str:
    # Surrogates range: U+D800 to U+DFFF
    # This function drops them or replaces with a placeholder
    return ''.join(c for c in s if not (0xD800 <= ord(c) <= 0xDFFF))


def process_year_file(year,
                      input_dir,
                      output_dir,
                      chunk_size=10_000,
                      max_workers=5):
    """
    Reads gdelt_{year}.parquet from 'input_dir', scrapes in chunks, and appends results to
    'gdelt_scraped_{year}.csv' in 'output_dir'.

    - chunk_size: number of rows to process per chunk (reduce if memory or speed is an issue)
    - max_workers: concurrency level for scraping
    """
    # Prepare paths
    input_path = Path(input_dir) / f"gdelt_{year}.parquet"
    output_path = Path(output_dir) / f"gdelt_scraped_{year}.csv"
    domain_file_path = Path(output_dir) / f"gdelt_scraped_{year}_domains.csv"

    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Check that input file exists
    if not input_path.exists():
        print(f"File not found: {input_path}")
        return

    # Load entire Parquet into memory if it's ~200MB (usually fine in free Colab).
    # If you have memory issues, consider reading the Parquet in an iterator-like approach.
    df = pd.read_parquet(input_path)
    total_rows = len(df)
    print(f"Year {year}: total rows = {total_rows}")

    start_idx = 0
    file_exists=output_path.exists()
    domain_file_exists = domain_file_path.exists()
    if file_exists:
        # If CSV exists, read its LAST ROW to get the last scraped URL
        # reading all might be big, so let's read only the last line
        # but Pandas doesn't have a built-in "tail(1)" without reading entire file.
        # For simplicity, let's just read the entire file's "SOURCEURLS" column.
        existing_df = pd.read_csv(output_path, usecols=["SOURCEURLS"], on_bad_lines="skip")
        if not existing_df.empty:
            last_scraped_url = existing_df["SOURCEURLS"].iloc[-1]
            # Find the LAST occurrence of that URL in our df
            indices = df.index[df["SOURCEURLS"] == last_scraped_url].tolist()
            if len(indices) > 0:
                last_idx = indices[-1]  # last occurrence
                # We'll resume from the row AFTER that index
                start_idx = last_idx + 1
                print(f"Resuming from row index {start_idx} (just after URL = {last_scraped_url})")

    pool = Pool(processes=max_workers, initializer=init_pool)
    # We'll iterate from start_idx -> total_rows in steps of chunk_size
    cur_idx = start_idx
    while cur_idx < total_rows:
        end_idx = min(cur_idx + chunk_size, total_rows)
        chunk_df = df.iloc[cur_idx:end_idx]
        records = chunk_df.to_dict("records")

        start_time = time.time()
        async_results = [pool.apply_async(scrape_article, (row,)) for row in records]

        results = []
        for async_result in async_results:
            try:
                result = async_result.get(timeout=5)  # Enforce strict 5s timeout
                results.append(result)
            except TimeoutError:
                results.append(None)

        # Filter out None values
        results = [r for r in results if r is not None]

        if not results:
            # If everything in this chunk failed, just move on
            print(f"Chunk rows {cur_idx}-{end_idx} => no successful scrapes")
            cur_idx = end_idx
            continue

        # Convert successful results to DataFrame
        results_df = pd.DataFrame(results)
        main_df = results_df[["DATE", "SOURCEURLS", "content"]]
        # Condition: content not null and content != ""
        mask = main_df["content"].notna() & (main_df["content"] != "")
        main_df = main_df[mask]

        domain_df = results_df[["SOURCEURLS", "domain", "time_taken", "status_code"]]

        # Append to CSV
        write_header_main = (not file_exists and cur_idx == 0)
        main_df.to_csv(
            output_path,
            mode='a',
            header=write_header_main,
            index=False
        )
        file_exists = True  # after the first write, definitely True

        # Uncomment to monitor domains for BLOCKED_DOMAINS list
        # write_header_domain = (not domain_file_exists and cur_idx == start_idx)
        # domain_df.to_csv(
        #     domain_file_path,
        #     mode='a',
        #     header=write_header_domain,
        #     index=False
        # )
        # domain_file_exists=True

        # Count how many total lines in CSV (for the consumer)
        # line_count = count_csv_data_lines(output_path)
        # write_progress(progress_path, line_count)
        end_time = time.time()
        elapsed = end_time-start_time
        print(f"Chunk rows {cur_idx}-{end_idx} => appended {len(results_df)} rows to {output_path} in {elapsed:.2f} seconds")

        # Move to the next chunk
        cur_idx = end_idx

    pool.close()
    pool.join()
    print(f"Finished processing year {year} -> {output_path}")

if __name__ == "__main__":
    process_year_file(
        year=2021,
        input_dir=DATASETS_DIR,
        output_dir=OUTPUT_DIR,
        chunk_size=5_000,
        max_workers=20
    )
